'''Our raw data is being sent to a blob storage.

Autoloader simplify this ingestion, including schema inference, schema evolution while being able to scale to millions of incoming files.

Autoloader is available in Python using the cloud_files format and can be used with a variety of format (json, csv, avro...):

STREAMING LIVE TABLE
Defining tables as STREAMING will guarantee that you only consume new incoming data. Without STREAMING, you will scan and ingest all the data available at once. See the documentation for more details
'''
import dlt
from pyspark.sql import functions as F
 
@dlt.create_table(comment="New raw loan data incrementally ingested from cloud object storage landing zone")
def raw_txs():
  return (
    spark.readStream.format("cloudFiles")
      .option("cloudFiles.format", "json")
      .option("cloudFiles.inferColumnTypes", "true")
      .load("/demos/dlt/loans/raw_transactions"))
@dlt.create_table(comment="Lookup mapping for accounting codes")
def ref_accounting_treatment():
  return spark.read.format("delta").load("/demos/dlt/loans/ref_accounting_treatment")
@dlt.create_table(comment="Raw historical transactions")
def raw_historical_loans():
  return (
    spark.readStream.format("cloudFiles")
      .option("cloudFiles.format", "csv")
      .option("cloudFiles.inferColumnTypes", "true")
      .load("/demos/dlt/loans/historical_loans"))
